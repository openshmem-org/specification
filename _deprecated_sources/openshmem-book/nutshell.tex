%
% Copyright (c) 2011, 2012
%   University of Houston System and Oak Ridge National Laboratory.
% 
% All rights reserved.
% 
% Redistribution and use in source and binary forms, with or without
% modification, are permitted provided that the following conditions
% are met:
% 
% o Redistributions of source code must retain the above copyright notice,
%   this list of conditions and the following disclaimer.
% 
% o Redistributions in binary form must reproduce the above copyright
%   notice, this list of conditions and the following disclaimer in the
%   documentation and/or other materials provided with the distribution.
% 
% o Neither the name of the University of Houston System, Oak Ridge
%   National Laboratory nor the names of its contributors may be used to
%   endorse or promote products derived from this software without specific
%   prior written permission.
% 
% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
% ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
% LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
% A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
% HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
% SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
% TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
% PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
% LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
% NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
% SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
%


\chapter{\openshmem in a Nutshell}

% A basic overview of \openshmem.  Introduce symmetric memory,
% one-sided communications, overlap, and explain some of the terms
% we'll be seeing later (e.g.\ collective, atomic).

In this chapter we will summarize the important concepts that underly
\openshmem.

\section{Remotely Accessible Symmetric Memory}

Distributed programming models partition a running program into
``processes''.  Processes have separate memory spaces that are private
to each process.\footnote{Contrast processes with threads that share a
  single memory space} PGAS models extend the memory space to include
areas of memory that can be accessed remotely.  In \openshmem these
are known as ``symmetric memory'' and are the areas that can be read
or written by get, put and other communication calls.

\openshmem provides \ref{itm:nkinds} kinds of remotely accessible memory:

\begin{enumerate}
  \item global variables pre-allocated at compile-time
  \item \label{itm:nkinds} dynamic allocations at execution-time
\end{enumerate}

Let us look at what each of these involves.

\subsection{Global Symmetric Variables}

Programming models make a distinction between ephemeral and persistent
data.

Subroutine or function calls allocate private data and parameters on
the process' stack, and this exists only as long as the subroutine or
function exists.  An interesting mistake is to return a pointer from a
C subroutine when that pointer has been allocated on the subroutine's
stack: upon return, the pointer will be invalid (although, depending
on what happens to that memory area, things may \emph{appear} to work
sometimes).

Persistent data is stored in persistent areas of the memory of the
process.  The way in which one specifies this is shown below for C and
Fortran:

\vspace{0.2in}
\begin{table}[h]
  \begin{center}
    \caption{Classes of Global Symmetric Variables}
    \begin{tabular}{|p{0.2\textwidth}|p{0.7\textwidth}|}
      \hline 
      Language & Symmetric Variable \tabularnewline
      \hline
      \hline 
      C & Global variables outside of any subroutine \tabularnewline
        & Variables declared \texttt{static} \tabularnewline
      \hline 
      Fortran & \texttt{COMMON} blocks \tabularnewline
              & Variables in main program \tabularnewline
              & Variables declared \texttt{SAVE} \tabularnewline
      \hline 
    \end{tabular}
  \end{center}
\end{table}
\vspace{0.2in}

Since the same process runs on each processing element (PE) in an
\openshmem program, global data is stored at the same location in each
process and thus is trivially symmetric\todo{At least, it is in the
  ELF format, need to look up some more, e.g. XCOFF, Mach-O}.

\subsection{Dynamically Allocated Symmetric Variables}

The reader may be familiar with the C language's \texttt{malloc}
family of calls or Fortran's \texttt{allocatable}.  Data allocated wby
such calls persists beyond the lifetime of the scope in which the
allocation occurred: a corresponding \texttt{free} or
\texttt{deallocate} call is used to return the allocated memory to the
system for further use.  Mismanagement of such dynamic allocation
frequently leads to memory leaks that cause crashes or slowdowns.

\openshmem extends the notion of dynamically managed memory with API
calls that allocate within a special area of memory that can be seen
by other processes.  The layout of this memory is identical across all
processes in an \openshmem program, which is why it is termed
``symmetric''.

It is important to note that \openshmem does not guarantee that the
symmetric data will necessarily be at exactly the same memory location
in each process, but only that the layout will be identical and that
each PE can address remote locations by referring to its local
symmetric variables.  Some implementations may be able to really use
the same memory locations but others may not, and will therefore need
to translate addreses used on one PE to those of another PE.  One
impediment is the use of Address Space Randomization~\cite{ASLR}.

% \begin{itemize}
% \item ``readiness' of symmetric memory.  This conflates 2 steps,
%   that the memory has been
%   \begin{enumerate}
%   \item allocated; and then
%   \item completely assigned a value
%   \end{enumerate}
% \end{itemize}


\section{Point-to-Point Communication}

\openshmem supports both remote writes (put) and reads (get) of data
on a target PE.  The active PE (the one that performs the put/get) is
usually not the same PE as the target, but there's no reason it can't
be.  There is an important difference in behavior between put and get.
A ``get'' can be thought of as a remote variable assignment: when the
subroutine returns, the local data has been updated and can be used.
However, in a ``put'', upon return we cannot assume that the data has
been written remotely: we need a subsequent synchronization call to
guarantee this.  This is useful because splitting the send and
completion phases allows us to overlap local computation with remote
communication (and thus do both at once!).

\openshmem allows us to transfer different sizes and arrangements of
data.  These are the following kinds of transfer (both put and
get):

\vspace{0.2in}
\begin{table}[h]
  \begin{center}
    \caption{Types of put/get Calls}
    \begin{tabular}{|p{0.2\textwidth}|p{0.7\textwidth}|}
    \hline
    single-value     & transfer from one source scalar variable to a target scalar \tabularnewline
    \hline
    contiguous       & transfer from one source array to a target array \tabularnewline
    \hline
    strided          & transfer from one array to another but with specified skip distances for both the source and target arrays \tabularnewline
    \hline
    \end{tabular}
  \end{center}
\end{table}
\vspace{0.2in}

Examples of these calls can be found in section~\ref{sec:p-to-p}.

The calls all have variants that correspond to the type of the
variables.  The precise names differ between C and Fortran to match
the type names present in those languages; in C, there are, amongst
others, put/get calls for int, long, and double; in Fortran, amongst
others, character, int4, real8.

\section{Synchronization}

\openshmem put calls proceed without knowing if the data has been
stored at the target.  This means we need a mechanism to ensure that
pending stores have in fact occurred.  \openshmem provides 2 calls for
this:

\subsection{Fence}

A \texttt{shmem\_fence} ensures that all puts to a particular PE will
complete before another put to that PE completes.  Puts to other PEs
that are in progress are not affected by fence.

Fence is a partial ordering.

\subsection{Quiet}

Quiet, on the other hand, imposes a complete ordering across all
pending put calls.

\texttt{shmem\_quiet} ensures that all puts to \emph{all} PEs complete
before any new put after the quiet completes.

\subsubsection{Demonstrating fence vs. quiet}

Let's suppose we have an \openshmem program with 4 PEs.  On PE 0, for
example, there are 3 outbound logical queues, one per other PE:

\todo{Obviously need proper fancy figure here}
\vspace{0.2in}
\begin{verbatim}
       1       2       3
      ---     ---     ---
     |   |   |   |   |   |
     |   |   |   |   |   |
     |   |   | X |   |   |
     | X |   | X |   |   |
     | X |   | X |   |   |
     | X |   | X |   |   |
     | X |   | X |   | X |
      ---     ---     ---
\end{verbatim}
\vspace{0.2in}

After a fence, one or more \texttt{shmem\_put}s to a particular PE
will cause the corresponding outbound queue to drain before the puts
complete.  Puts to other PEs are not affected.

After a quiet, one or more \texttt{shmem\_put}s to \emph{any} PE will
cause \emph{all} outbound queues to drain before the puts complete.

\subsubsection*{Implementation Note}

Libraries may choose to implement \texttt{shmem\_fence} as
\texttt{shmem\_quiet} if this is more efficient for a particular
configuration.

The reverse is not true, though: \texttt{shmem\_quiet} cannot be
implemented as \texttt{shmem\_fence} because quiet has stronger
semantics than fence.

It may be that the interconnect hardware only manages a single,
tagged, out-bound queue for puts, and scanning it to identify traffic
to a particular PE is more expensive than simply blocking the queue as
a whole untll it drains.

Hardware that manages a number of out-bound queues can simply drain
the affected queue in a fence, leaving the other queue(s) free to
continue communication.

\section{Collective Communication}

In many programs, it is necessary for some data to be made available
in a number of places, or for distributed data to be collected and
processed in some way. \openshmem provides routines that operate
``collectively'', that is, they involve a number of PEs acting in
concert, called an ``active set''.  Currently \openshmem specifies
active sets through a ``triple'':

\begin{enumerate}
\item start PE
\item stride (gap between PEs in the active set)
\item size (number of PEs in the active set)
\end{enumerate}

Collective routines do not return until all PEs involved have finished
and data held locally has been stored.

\subsection{Barriers}

% barrier\_all and barrier(active set).

A barrier call is a mechanism by which a number of PEs (an active set)
must wait for all those PEs to enter the barrier, before any can
leave.  \openshmem provides a barrier call that operates on a named
active set and also a global barrier \texttt{shmem\_barrier\_all} that
applies unconditionally to all PEs in the program.  The global barrier
ensures a flush (see quiet) of pending \openshmem communication before
continuing.

A barrier, or other synchronization mechanism, is required before
accessing symmetric data that has been initialized.  E.g.\ before
calling a collective that uses a synchronization array for the first
time, that array must have been initialized: all PEs calling the
collective must have initialized locally before entering the
collective, otherwise some PEs may enter and remotely access arrays
containing abitrary values on other PEs.  This can lead to undefined
behavior where a PE believes it can proceed before data is ready
elsewhere: a sum reduction may then generate incorrect results.

\subsection{Broadcasts}

A broadcast is a collective call that transfers data from one PE to
all the othere in a given active set.  The active set is specified as
usual, plus one of the PEs in that set is named as a ``root''.  The
contiguous data from the source array on the root is then sent to the
target arrays on the other active set PEs (but \emph{not} on the root
itself).

\subsection{Collects}

A ``collect'' performs concatenation of data from a source array into
a target array on an active set of PEs.  The concatenation is
performed in PE-order, i.e.\ the data from the first PE in the active
set is written first into the target, then data from the 2nd PE, and
so on.  Each PE may contribute different amounts of data to the
target.

In the special case where it is known that all PEs in the active set
will write \emph{exactly} the same amount of data, a variant
``fcollect'' call (``f'' standing variously for ``fixed'' or ``fast'':
take your pick) exists.  This variant allows the implementation to be
more efficient because all PEs in the active set know in advance where
to write their data.

After the collect, all participating PEs have a copy of the
concatenated data.

\subsection{Reductions}

A reduction is an associative binary operation.  A well-known example
of this is ``factorial'' which multiplies a set of values together
into a single result:

\vspace{0.2in}

\begin{math}
n! = \prod_{i=1}^n i
\end{math}

\vspace{0.2in}

\noindent
So for example

\vspace{0.2in}

\begin{math}
6! = 6 * 5 * 4 * 3 * 2 * 1
\end{math}

\vspace{0.2in}

\noindent
Note that the order in which the multiplication occurs does not
matter.

The reductions in \openshmem operate across arrays of values on PEs in an
active set.  Each PE contributes one value for each reduction, and the
results are available on all PEs in the active set.

\subsubsection{Minimum and Maximum}

These 2 routines have versions for various different numeric types
(integer and floating-point) and find the smallest (min) and largest
(max) value in the source arrays.  For example:

\vspace{0.2in}

\begin{math}
max(2, 4, 6, 12, 4, 2, -6, 7, 9, 12, 8, 3) = 12
\end{math}

\subsubsection{Arithmetic}

These 2 routines have versions for various different numeric types
(integer and floating-point) and find the sum and product (see the
above factorial example) of the source arrays.  For example:

\vspace{0.2in}

\begin{math}
sum(6, 7, 8, 9) = 30
\end{math}

\subsubsection{Logical}

These 2 routines have versions for various different numeric types
(integer and complex) and perform the following Boolean logic
operations:

\begin{itemize}
\item or
\item and
\item exclusive or (xor)
\end{itemize}

The operations are logical rather than bitwise, so the source value 0
corresponds to ``false'', anything else to ``true''.  For example:

\vspace{0.2in}

\begin{minipage}{\linewidth}
\begin{math}
or(32, 16, 0, 8) = 1
\end{math}

\begin{math}
and(32, 16, 4) = 1
\end{math}

\begin{math}
and(32, 0, 4) = 0
\end{math}
\end{minipage}

\section{Atomic Operations}

An ``atomic memory operation'' (AMO) is an assignment to a variable
that occurs without interruption: 2 atomic operations cannot update
the same variable at the same time: we may not know in which order the
updates happen, but we know that both will occur completely and
separately.

The AMOs in \openshmem are remote, and address symmetric variables
only.

\subsection*{Caveat}

The guarantee of atomicity applies only to interaction with other
AMOs.  An unguarded put may write to a variable that is engaged in an
AMO at any time, and an operation executing outside of \openshmem can
also update memory~\footnote{A multithreaded program may have one
  thread running \openshmem and other threads doing other work}.  This
restriction is important for performance reasons on network interfaces
that offload certain operations from the CPU onto the interface.

\subsection{Swaps}

These AMOs take a new value, write it to a target variable and return
the previous value of that target variable.  One call does this
unconditionally, another takes an extra ``compare'' value and the swap
only occurs if the provided value is equal to the target variable's
value.

\todo{Swaps: real usage of these?}

\subsection{Add and Increment}

These AMOs add a constant to the value of a variable on a target PE.
The constant can be provided by the programmer in the add operation,
or is 1 in increment.~\footnote{Increment appears to be just a special
  case of ``add'' but certain hardware may provide dedicated
  instructions for increment that allow implementers to optimize.}

\subsection{Fetch, with Add and Increment}

These AMOs are the same as ``add and increment'' described above, but
they also return the previous value of the target variable to the
caller.

\subsection{Distributed Locks}

Distributed locks provide a mechanism to implement mutual exclusion or
critical sections.  A lock applies to a symmetric variable: a PE that
enters a lock call guarded by a variable will be the only PE active
amongst other PEs also trying to acquire the lock.  Thus a lock will
force the participating PEs to serialize their execution of the locked
section.

An application involving, for example, a distributed hash table
\todo{DHT: ref to Maynard/Nakao submission} would update its contents
consistently by locking access to an index, so that only one PE can
manipulate the index at a time.

Locks can seriously impact the performance of applications as PEs
often sit idle while waiting to acquire a lock.  If locks can be
avoided, they should be.  If not, the work done within a lock should
be as small and as quick as possible to minimize the overhead of the
lock.
