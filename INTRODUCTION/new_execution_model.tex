%Outline
%%Exectution model
%   *Define what is a OpenSHMEM program: a set of processes (either SPMD or MIMD?) where each process has its own 'local' (private) memory and symmetric memory regions that may be accessible by any PEs.
%   *Each OpenSHMEM process is called a processing element (PE)
%   *Each PE may be mapped to many to one hardware cores/threads or less.
%   *The number of PEs is specified at launch/runtime.
%   *Each PE must call startpe to initialize the OpenSHMEM runtime, before any other call for OpenSHMEM. There is an implicit barrier at startpe.
%   *Each PE executes asynchronously following Fortran or program execution in C [ISO/IEC00 Sec. 5.1.2.3]
%   *Each PE will have a unique global identifier and the execution of a program may depend on the PE id, if executed in SPMD.
%   *PE id may be used for library calls synchronizations, control flow constructs language  in C/Fortran
%   *PE may allocate symmetric data objects via a symmetric heap  during execution%SP: Does not cover global and static.
%   *As of now, PEs may finish execution at any time by returning from the main function. (no call to shmem_finalize yet!)
%   
%This comes from the UPC spec:
%The memory consistency model in a language defines the order in which the results of write operations may be observed through read operations.
%The behavior of a OpenSHMEM program may depend on the timing of accesses to symetric variables on PEs, so in general a program defines a set of possible executions, 
%rather than a single execution. The memory consistency model constrains the set of possible executions for a given program; the user may then rely 
%on properties that are true of all of those executions.
    

\section{Execution Model}
\label{subsec:execution_model}
%An \openshmem{} program consists of a set of processes, called \ac{PE}s, that execute in a \ac{SPMD}-like execution model. In \openshmem different \ac{PE}s can have different execution paths and will execute asynchronously 
%following \Fortran{} or \Clang{} program execution. The \ac{PE}s progress independently, and can communicate and synchronize using the \openshmem{} \ac{API}.
%The number of \ac{PE}s in the \openshmem{} program is specified at runtime by the user. %A \ac{PE} can be implemented as an OS process or OS thread~\footnote{As long as the memory model and execution model of \openshmem is followed.}. 
%The total number of \ac{PE}s, \VAR{N}, can be mapped to \VAR{M} hardware cores/threads where \VAR{M} can be less or equal than \VAR{N}. %As long as the memory model and execution model of \openshmem is followed.
%An \openshmem program must start by calling the initialization function \FUNC{start\_pes}  before using any of the other \openshmem library routines. \ac{PE}s do not exist until after the call to \FUNC{start\_pes} returns. 
%During execution, each \ac{PE} is assigned a unique global identifier for the duration of the program. These \ac{PE} identifiers are integers assigned in a monotonically increasing manner from zero to the total number of \ac{PE}s minus 1. \ac{PE} identifiers are used for other \openshmem library calls (e.g. to access symmetric objects from specific \ac{PE}s, collective synchronization) or to dictate a control flow for \ac{PE}s using constructs of 
%\Clang{} or \Fortran.  As of now, an \openshmem program finishes execution by returning from the main function. It is up to the implementation on how to handle the finalization of the \openshmem library and any other 
%resources initialized by the library: there is currently no explicit finalization call defined in the \openshmem specification.

An \openshmem{} program consists of a set of \openshmem{} processes called \ac{PE}s that execute in a \ac{SPMD}-like model where each \ac{PE} can take a different execution path. A \ac{PE} can be implemented using an OS process or an OS thread\footnote{\cbstart However, implementing a \ac{PE} using an OS thread requires compiler techniques to implement the \openshmem{} memory model.\cbend}.
The \ac{PE}s progress asynchronously, and can communicate/synchronize 
via the \openshmem{} interfaces. All \ac{PE}s in an \openshmem{} program should start by calling the initialization function \FUNC{start\_pes} before using any of the other \openshmem{} library routines. As of now, an \openshmem program finishes execution by returning from the main function. On program exit, \openshmem \cbstart must complete all pending communication and release all the resources associated to the library using an implicit collective synchronization across \ac{PE}s.\cbend
%On program exit, \openshmem can release all the resources associated to the library. 
%  It is up to the implementation on how to handle the finalization of the \openshmem library. 

The \ac{PE}{}s of the \openshmem{} program are identified by unique integers. The identifiers are integers assigned in a monotonically increasing manner from zero to the total number of \ac{PE}s minus 1. \ac{PE} identifiers are used for \openshmem{} calls (e.g. to specify \PUT{} or \GET{} operations on symmetric data objects, collective synchronization calls, etc.) or to dictate a control flow for \ac{PE}s using constructs of \Clang{} or \Fortran. The identifiers are fixed for the life cycle of the \openshmem{} program.
%on exit implementation are expected to release resources associated to the library
  %following \Fortran{} or \Clang{} program execution.
%Each \ac{PE} can be implemented as an OS process or OS thread as long as the constraints imposed
%by the memory model and execution model are respected. 
%The \ac{PE} in turn is mapped to either a processor core or a hardware thread. \rcomment{Manju: This sentence requires scrutiny}. 
%Though all \ac{PE}s are required to execute the same program, each \ac{PE} is allowed to take 
%a different control path. The \ac{PE}s progress independently, and can communicate and synchronize using the \openshmem{} interfaces.

%The life cycle of \openshmem{} program starts with each \ac{PE} calling a global
%collective routine start\_pes{}, and ends with implementation dependent
%finalization. 
%All \ac{PE}s in an \openshmem{} program should start by calling
%the initialization function start\_pes before using any of the other \openshmem{}
%library routines. A \ac{PE}{} calling start\_pes{} call more than once in the lifetime of program can result in an undefined behavior. The current specification does not define the finalization of \openshmem{}program. The implementations are allowed to provide their own interfaces for finalization as long as it is not required for the correct functioning of the \openshmem{} program.  
%The \ac{PE}{}s of the \openshmem{} program are identified by unique integers.
%The identifiers are integers assigned in a monotonically increasing manner from zero to the total number of \ac{PE}s minus 1. \ac{PE} identifiers are used for other \openshmem{} library calls (such as collective synchronization) or to dictate a control flow for \ac{PE}s using constructs of \Clang{} or \Fortran. The identifiers are fixed for the life cycle of the \openshmem{} program.
%on exit implementation are expected to release resources associated to the library

\subsection{Progress of \openshmem operations}
\label{subsec:progress}
The \openshmem model assumes that computation and communication are naturally \cbstart overlapped. \openshmem programs are expected to exhibit progression of communication both with and without \openshmem calls. Consider a \ac{PE} that is engaged in a computation with no \openshmem calls. Other \ac{PE}s should be able to communicate (\OPR{put}, \OPR{get}, \OPR{collective}, \OPR{atomic}, etc) and complete communication operations with that computationally-bound \ac{PE} without that \ac{PE} issuing any explicit \openshmem calls. \openshmem communication calls involving that \ac{PE} should progress regardless of when that \ac{PE} next engages in an \openshmem call.\cbend

\textbf{Note to implementors:}

\cbstart \begin{itemize}
\item An \openshmem implementation for hardware that does not provide asynchronous communication capabilities may require a software progress thread in order to progress remotely-issued communication requests without explicit application calls to the \openshmem library.
\item High performance implementations of \openshmem are expected to leverage hardware offload capabilities and
    provide asynchronous one-sided communication without software assistance.
\item Implementations should avoid deferring the execution of one-sided operations until a synchronization point where data is known to be available. High-quality implementations should attempt asynchronous delivery whenever possible, for performance reasons. Additionally, the \openshmem community discourages releasing \openshmem implementations that do not provide asynchronous one-sided operations, since those have very limited performance value for \openshmem applications.
\end{itemize} \cbend

%
%ORIGINAL TEXT
%The \openshmem model assumes that computation and communication are
%naturally overlapped, and that all data transfers eventually complete. The OpenSHMEM execution model assumes that computation and communication are naturally overlapped. 
%OpenSHMEM programs are expected to exhibit progression of communication both with and without OpenSHMEM calls.
%
%\textbf{Note to implementors:} while delivery can be deferred, for example until a synchronization point at which data is known to be available, high-quality implementations should attempt asynchronous delivery, whenever possible, for performance reasons. Progress can be ensured through the use of a dedicated progress thread in software, or through network hardware that offloads communication handling from processors, for example.

%High quality \openshmem implementations must insure that programs exhibit %SP: Changing MUST to may as per discussion on 01/31/2014
%progression of communication both with and without \openshmem calls.

% A high quality \openshmem{} implementation may ensure that communication will
% progress without requiring \openshmem{} calls. 

%Consider a \ac{PE} that is engaged in a long computation with no \openshmem calls: other \ac{PE}s must be able to communicate (e.g. \PUT{}/\GET{},
%collective, atomic operations) with that computationally-bound \ac{PE} without that \ac{PE}
%issuing any explicit \openshmem calls. \openshmem communication calls involving that \ac{PE} must progress
%regardless of when that \ac{PE} next engages in an \openshmem call.

%\textbf{Note to implementers:} progress will often be ensured through
%the use of a dedicated progress thread in software, or through
%network hardware that offloads communication handling from processors, for example.
%SP: Why only communication ? Shouldn't t be for all openshmem calls?
% TC: separating out communication seemed to be the relevant comment here, as
%     opposed to other offloads
%\subsection{Using the Symmetric \VAR{Work} and \VAR{pSync} Arrays}

%Multiple \VAR{pSync} arrays are often needed if a particular \ac{PE} calls a \openshmem
%collective  function twice without intervening barrier synchronization.
%Problems would occur if some \ac{PE}s in the \activeset{} for call 2 arrive at
%call 2 before processing of call 1 is complete by all \ac{PE}s in the call 1
%\activeset.  You can use  \FUNC{shmem\_barrier}  or  \FUNC{shmem\_barrier\_all}  to
%perform  a  barrier  synchronization between consecutive calls to \openshmem
%collective functions. There are two special cases:
%\begin{itemize}
%\item The \FUNC{shmem\_barrier} function allows the same \VAR{pSync} array to be used
%          on consecutive calls as long as the active \ac{PE} set does not change.
%\item  If the same collective function is called multiple times with the
%          same \activeset, the calls may alternate between two \VAR{pSync} arrays.
%          The \openshmem functions guarantee that a first call is completely finished by 
%          all \ac{PE}s by the time processing of a third  call  begins  on
%          any \ac{PE}.          
%\end{itemize}
%Because  the \openshmem functions restore \VAR{pSync} to its original contents,
%multiple calls that use the same \VAR{pSync} array do not require that \VAR{pSync}
%be reinitialized after the first call.

\subsection{Atomicity Guarantees}
\label{sec:amo_guarantees}
\openshmem contains a number of routines that operate on symmetric data
atomically (Section \ref{sec:amo}).  These routines guarantee that accesses by \openshmem's atomic operations will be exclusive, but do not guarantee exclusivity
in combination with other routines, either inside \openshmem or outside.

For example: during the execution of \cbstart an \cbend atomic remote integer increment
operation on a symmetric variable \VAR{X}, no other \openshmem atomic
operation may access \VAR{X}.  After the increment, \VAR{X} will have
increased its \cbstart value \cbend by \CONST{1} on the \target{} \ac{PE}, at which point other
atomic operations may then modify that \VAR{X}.
However, access to the symmetric object \VAR{X} with non-atomic operations, such as one-sided \PUT{} or \GET{} operations, will \OPR{invalidate} the atomicity guarantees.

%  %Memory model
%    *Each OpenSHMEM PEs may have symmetric memory that is accessible by other PEs. 
%    *Symmetric memory is a region of memory where all the an instance of a data objects is replicated across PEs, have 
%     the same the same layout and relative offset.
%    *All PEs can allocate a symmetric data objects using the symmetric heap, but they must do so as a collective operation. (is there a barrier after shmalloc?)
%    *All writes to symmetric memory are relaxed (I'm not sure if this is the completion semantics) and are guaranteed to be visible to other PEs after a barrier_all, barrier(?), quiet, (what about wait? does it means iti sonly visible to me?) 
%    *Calls to barrier, barrier_all, quiet, wait, lock, atomics, are meant to guarantee memory consistency across PEs.
%    *Read/Writes to symmetic data object may appear after startpe or after a the symmetric data object has been allocated in the symmetric heap (if it is a dynamic).
%    *Operations like reduction, collect, etc guarantee memory consistency after completion(?)
%    *Data races are possible in OpenSHMEM if multiple PEs write/read a symmetric data object from a single PE without proper synchronization.  
