\section{Programming Model Overview}
%SP: Addressing suggestions from discussion on 01/28/2014 Merging the commented portions into the body. 
%The \openshmem programming model consists of library routines that provide
%low-latency, high-bandwidth communication  for  use  in  highly  parallelized 
%scalable programs. The routines in the \openshmem \ac{API} provide a programming 
%model for exchanging data between cooperating parallel processes. The resulting programs are similar 
%in style to \ac{MPI} programs. The \openshmem \ac{API} can be used either alone 
%or in combination with \ac{MPI} routines in the same parallel program.
%\openshmem implements a \ac{PGAS} model. 
%In the \ac{PGAS} model, each process has a local and globally shared memory where portions of the shared memory may have affinity to a particular process. 
\openshmem implements \ac{PGAS} by defining remotely accessible data objects as mechanisms to share information among \openshmem processes or \acp{PE} and private data objects that are accessible by the \ac{PE} itself. The \ac{API} allows communication and synchronization operations on both private (local to the PE initiating the operation) and remotely accessible data objects. The key feature of \openshmem is that data transfer operations are \textit{\textbf{one-sided}} in nature. This means that a local \ac{PE} executing a data transfer does not require the participation of the remote \ac{PE} to complete the operation. This allows for overlap between communication and computation to hide data transfer latencies, which makes  \openshmem ideal for unstructured, small/medium size data communication patterns. The \openshmem library routines have the potential to provide low-latency, high-bandwidth communication \ac{API} for use in highly parallelized scalable programs.  
%\rcomment{Manju: To do - Make sure the first paragraph does not say the same
%things has first paragraph of Section 1}
%An \openshmem program is currently \ac{SPMD} in style. The
%\openshmem  processes, called \ac{PE}s, all start at the
%same time, and they all run the same program. Usually the \ac{PE}s perform
%computation on their own subdomains of the larger problem, and periodically 
%communicate with other \ac{PE}s to exchange information on which the
%next computation phase depends.
%SP: Addressing suggestions from discussion on 01/31/2014

%Data latency is  the  period  of  time that starts when a \ac{PE} initiates a transfer of data 
%and ends when a \ac{PE} can use the data. %SP: What about put? Not guaranteed till synchronization is hit.

%SP: Addressing suggestions from discussion on 01/28/2014
%\openshmem routines support remote data transfer through \PUT operations, which  transfer data to a 
%different \ac{PE}, get operations, which transfer data from a different \ac{PE}, and remote pointers, which 
%allow direct  references  to  data objects owned by another \ac{PE}. Other operations supported are \OPR{collective} 
%\OPR{broadcast} and \OPR{reduction}, \OPR{barrier synchronization}, and \OPR{atomic memory operations}. 
%An atomic memory operation  is an atomic read-and-update operation, such as a fetch-and-increment, on a remote
%or local data object.

%\rcomment{Manju: [The idea is to talk SPMD. We are talking about nature of interfaces rather than 
% the interfaces that enable SPMD. Replace the second paragraph with the one below ?]}
%\rcomment{\\ Oscar: I'm good with this change. minor change to say the SPMD can be used to decompose work\\}
The \openshmem{} interfaces can be used to implement \ac{SPMD} style programs. It provides interfaces to start the \openshmem{} \ac{PE}s in parallel, and communication and synchronization interfaces to access remotely accessible data objects across \ac{PE}s. These interfaces can be leveraged to divide a problem into multiple sub-problems that can solved independently or with coordination using the communication and synchronization interfaces.
The \openshmem specification defines library calls, constants, variables, and language bindings for \Clang{} and \Fortran{}.
The \Cpp{} interface is currently the same as that for \Clang. Unlike UPC, Fortran 2008, Titanium, X10 and Chapel, which are all PGAS languages, \openshmem relies on 
the user to use the library calls  to implement the correct semantics of its programming model.

An overview of the \openshmem routines is described below:

\begin{enumerate}
\item \textbf{Library Setup and Query}

\begin{enumerate}
\item \OPR{Initialization}: The \openshmem library environment is initialized. 
\item \OPR{Query}: The local \ac{PE} may get number of \acp{PE} running the same program and its unique integer identifier. 
\item \OPR{Accessibility}: The local \ac{PE} can find out if a remote \ac{PE} is executing the same binary, or if a particular symmetric data object can be accessed by a remote \ac{PE}, or may obtain a pointer to a symmetric data object on the specified remote \ac{PE} on shared memory systems.
\end{enumerate}

\item \textbf{Symmetric Data Object Management}
\begin{enumerate}
\item \OPR{Allocation}: All executing \ac{PE}s must participate in the allocation of a symmetric data object with identical arguments.
\item  \OPR{Deallocation}: All executing \ac{PE}s must participate in the deallocation of the same symmetric data object with identical arguments.
\item  \OPR{Reallocation}: All executing \ac{PE}s must participate in the reallocation of the same symmetric data object with identical arguments.
\end{enumerate}

\item \textbf{Remote Memory Access}

\begin{enumerate}
\item \PUT: The local \ac{PE} specifies the \source{}
data (private or symmetric) that is copied to the symmetric data object on the remote \ac{PE}. 
\item \GET: The local \ac{PE} specifies the symmetric data object on the remote \ac{PE}
that is copied to a data object (private or symmetric) on the local \ac{PE}. 
\end{enumerate}

\item \textbf{Atomics}
\begin{enumerate}
\item \OPR{Swap}: The \ac{PE} initiating the swap gets the old value of the symmetric data object it is copying a new value to on the remote \ac{PE}.
\item \OPR{Increment}: The \ac{PE} initiating the increment adds 1 to the symmetric data object on the remote \ac{PE}.
\item \OPR{Add}: The \ac{PE} initiating the add specifies the value to be added to the symmetric data object on the remote \ac{PE}.
\item \OPR{Compare and Swap}: The \ac{PE} initiating the swap gets the old value of the symmetric data object based on a value to be compared and copies a new value to the symmetric data object on the remote \ac{PE}.
\item \OPR{Fetch and Increment}: The \ac{PE} initiating the increment adds 1 to the symmetric data object on the remote \ac{PE} and returns with the old value.
\item \OPR{Fetch and Add}: The \ac{PE} initiating the add specifies the value to be added to the symmetric data object on the remote \ac{PE} and returns with the old value.
\end{enumerate}

\item \textbf{Synchronization and Ordering}
\begin{enumerate}
\item \OPR{Fence}: The \ac{PE} calling fence ensures ordering of remote access operations and stores to symmetric data objects with respect to a specific destination \ac{PE}. 
\item \OPR{Quiet}: The \ac{PE} calling quiet ensures completion of remote access operations and stores to symmetric data objects. 
\item \OPR{Barrier}: All or some \ac{PE}s collectively synchronize and ensure completion of all remote and local updates prior to any \ac{PE} returning from the call.
\end{enumerate}

\item \textbf{Collective Communication}
\begin{enumerate}
\item \OPR{Broadcast}: The \textit{root} \ac{PE} specifics a symmetric data object to be copied to a symmetric data object on one or more remote \acp{PE} (not including itself). 
\item \OPR{Collection}: All \acp{PE} participating in the routine get the result of concatenated symmetric objects contributed by each of the \acp{PE} in another symmetric data object.
\item \OPR{Reduction}: All \acp{PE} participating in the routine get the result of an associative binary routine over elements of the specified symmetric data object on another symmetric data object. 
\end{enumerate}

\item \textbf{Mutual Exclusion}
\begin{enumerate}
\item \OPR{Set Lock}: The \ac{PE} acquires exclusive access to the region bounded by the symmetric \textit{lock} variable.
\item \OPR{Test Lock}: The \ac{PE} tests the symmetric \textit{lock} variable for availability.
\item \OPR{Clear Lock}: The \ac{PE} which has previously acquired the \textit{lock} releases it.
\end{enumerate}

\item \textbf{Data Cache Control \textit{(deprecated on cache coherent systems )}}
\begin{enumerate}
\item Implementation of mechanisms to exploit the capabilities of hardware
cache if available.
\end{enumerate}
\end{enumerate}

%\begin{description}
%\item [{{Note:}}] More information about \openshmem routines can be found
%in the Library Routines section.
%\end{description}
